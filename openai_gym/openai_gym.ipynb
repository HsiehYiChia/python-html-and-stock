{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym Cartpole-v0 ##\n",
    "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0  \n",
    "https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "## Initialize the \"Cart-Pole\" environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "## Defining the environment related constants\n",
    "\n",
    "# Number of discrete states (bucket) per state dimension\n",
    "NUM_BUCKETS = (1, 1, 6, 3)  # (x, x', theta, theta')\n",
    "# Number of discrete actions\n",
    "NUM_ACTIONS = env.action_space.n # (left, right)\n",
    "# Bounds for each discrete state\n",
    "STATE_BOUNDS = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "STATE_BOUNDS[1] = [-0.5, 0.5]\n",
    "STATE_BOUNDS[3] = [-math.radians(50), math.radians(50)]\n",
    "\n",
    "## Creating a Q-Table for each state-action pair\n",
    "q_table = np.zeros(NUM_BUCKETS + (NUM_ACTIONS,))\n",
    "print (\"Q table shape: \", q_table.shape)\n",
    "\n",
    "## Learning related constants\n",
    "MIN_EXPLORE_RATE = 0.01\n",
    "MIN_LEARNING_RATE = 0.1\n",
    "MIN_DISCOUNT_FACTOR = 0.96\n",
    "\n",
    "## Defining the simulation related constants\n",
    "NUM_EPISODES = 1000\n",
    "MAX_T = 250\n",
    "STREAK_TO_END = 120\n",
    "SOLVED_T = 199\n",
    "DEBUG_MODE = False\n",
    "\n",
    "\n",
    "def simulate():\n",
    "\n",
    "    ## Instantiating the learning related parameters\n",
    "    learning_rate = get_learning_rate(0)\n",
    "    explore_rate = get_explore_rate(0)\n",
    "    discount_factor = get_discount_factor(0)\n",
    "    #discount_factor = 0.99  # since the world is unchanging\n",
    "\n",
    "    num_streaks = 0\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "\n",
    "        # Reset the environment\n",
    "        obv = env.reset()\n",
    "\n",
    "        # the initial state\n",
    "        current_state = state_to_bucket(obv)\n",
    "\n",
    "        for t in range(MAX_T):\n",
    "            #env.render()\n",
    "\n",
    "            # Select an action\n",
    "            action = select_action(current_state, explore_rate)\n",
    "\n",
    "            # Execute the action\n",
    "            obv, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Observe the result\n",
    "            next_state = state_to_bucket(obv)\n",
    "\n",
    "            # Update the Q based on the result\n",
    "            q_table[current_state + (action,)] = (1 - learning_rate) * q_table[current_state + (action,)] + \\\n",
    "                                                  learning_rate * (reward + discount_factor*np.amax(q_table[next_state]))\n",
    "\n",
    "            # Setting up for the next iteration\n",
    "            current_state = next_state\n",
    "\n",
    "            # Print data\n",
    "            if (DEBUG_MODE):\n",
    "                print(\"\\nEpisode = %d\" % episode)\n",
    "                print(\"t = %d\" % t)\n",
    "                print(\"Action: %d\" % action)\n",
    "                print(\"State: %s\" % str(state))\n",
    "                print(\"Reward: %f\" % reward)\n",
    "                print(\"Best Q: %f\" % best_q)\n",
    "                print(\"Explore rate: %f\" % explore_rate)\n",
    "                print(\"Learning rate: %f\" % learning_rate)\n",
    "                print(\"Streaks: %d\" % num_streaks)\n",
    "\n",
    "                print(\"\")\n",
    "\n",
    "            if done:\n",
    "               print(\"Episode %d finished after %d time steps\" % (episode, t))\n",
    "               if (t >= SOLVED_T):\n",
    "                   num_streaks += 1\n",
    "               else:\n",
    "                   num_streaks = 0\n",
    "               break\n",
    "\n",
    "            #sleep(0.25)\n",
    "\n",
    "        # It's considered done when it's solved over 120 times consecutively\n",
    "        if num_streaks > STREAK_TO_END:\n",
    "            break\n",
    "\n",
    "        # Update parameters\n",
    "        explore_rate = get_explore_rate(episode)\n",
    "        learning_rate = get_learning_rate(episode)\n",
    "        discount_factor = get_discount_factor(episode)\n",
    "    \n",
    "    env.render(close=True)\n",
    "\n",
    "\n",
    "def select_action(state, explore_rate):\n",
    "    # Select a random action\n",
    "    if random.random() < explore_rate:\n",
    "        action = env.action_space.sample()\n",
    "    # Select the action with the highest q\n",
    "    else:\n",
    "        action = np.argmax(q_table[state])\n",
    "    return action\n",
    "\n",
    "\n",
    "def get_explore_rate(t):\n",
    "    return max(MIN_EXPLORE_RATE, min(1, 1.0 - math.log10((t+1)/25)))\n",
    "\n",
    "def get_learning_rate(t):\n",
    "    return max(MIN_LEARNING_RATE, min(0.5, 1.0 - math.log10((t+1)/25)))\n",
    "\n",
    "def get_discount_factor(t):\n",
    "    return max(MIN_DISCOUNT_FACTOR, min(0.99, 1.0 - math.log10((t+1)/25)))\n",
    "\n",
    "def state_to_bucket(state):\n",
    "    bucket_indice = []\n",
    "    for i in range(len(state)):\n",
    "        if state[i] <= STATE_BOUNDS[i][0]:\n",
    "            bucket_index = 0\n",
    "        elif state[i] >= STATE_BOUNDS[i][1]:\n",
    "            bucket_index = NUM_BUCKETS[i] - 1\n",
    "        else:\n",
    "            # Mapping the state bounds to the bucket array\n",
    "            bound_width = STATE_BOUNDS[i][1] - STATE_BOUNDS[i][0]\n",
    "            normalize_state = (state[i]-STATE_BOUNDS[i][0])*(NUM_BUCKETS[i]-1) / bound_width\n",
    "            bucket_index = int(round(normalize_state))\n",
    "        bucket_indice.append(bucket_index)\n",
    "    return tuple(bucket_indice)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    simulate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A painless Q-learning tutorial ##\n",
    "\n",
    "A simple implementation of Q-learning,  \n",
    "refer to this article : http://blog.csdn.net/pi9nc/article/details/27649323  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Q = np.zeros((6,6))\n",
    "R = np.array([[-1,-1,-1,-1,0,-1], \n",
    "             [-1,-1,-1,0,-1,100],\n",
    "             [-1,-1,-1,0,-1,-1],\n",
    "             [-1,0,-0,-1,0,-1],\n",
    "             [0,-1,-1,0,-1,100],\n",
    "             [-1,-0,-1,-1,0,100]])\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.8\n",
    "DEBUG_MODE = False\n",
    "\n",
    "\n",
    "def get_random_action(state):\n",
    "    action = np.random.randint(6)\n",
    "    while R[state][action] == -1:\n",
    "        action = np.random.randint(6)\n",
    "    return action\n",
    "\n",
    "\n",
    "def training():\n",
    "    for episode in range(100):\n",
    "        if(DEBUG_MODE):\n",
    "            print (\"EPISODE \", episode)\n",
    "        \n",
    "        # get initial state\n",
    "        state = np.random.randint(5)\n",
    "        while (state != 5):\n",
    "            action = get_random_action(state)\n",
    "            Q[state][action] = R[state][action] + LEARNING_RATE * np.amax(Q[action])\n",
    "            if(DEBUG_MODE):\n",
    "                print (\"Room %d --> Room %d\" % (state, action))\n",
    "            state = action\n",
    "            \n",
    "        if(DEBUG_MODE):\n",
    "            print (\"\")\n",
    "\n",
    "    print (\"Q table:\")\n",
    "    print (Q)\n",
    "    \n",
    "\n",
    "def verify():\n",
    "    for episode in range(10):\n",
    "        print (\"EPISODE \", episode)\n",
    "        state = np.random.randint(5)\n",
    "        print (\"Initial state\", state)\n",
    "        while(state!=5):\n",
    "            action = np.argmax(Q[state])\n",
    "            print (\"Room %d --> Room %d\" % (state, action))\n",
    "            state = action\n",
    "        \n",
    "        print(\"\")\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    training()\n",
    "    verify()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
